import keras
from keras import optimizers
from keras.layers import Activation
from keras import losses
from keras import metrics

import sys
import argparse
import numpy as np
import random
import gym
import pdb
import json


class Imitation():
    def __init__(self, model_config_path, expert_weights_path):
        # Load the expert model.
        with open(model_config_path, 'r') as f:
            self.expert = keras.models.model_from_json(f.read())
            self.expert_copy = self.expert
        self.expert.load_weights(expert_weights_path)
        
        # Initialize the cloned model (to be trained).
        with open(model_config_path, 'r') as f:
            self.model = keras.models.model_from_json(f.read())

        # Create the environment.
        self.env = gym.make('LunarLander-v2')
        self.num_obs = self.env.observation_space.shape[0]
        self.num_acts = 4

        # model
        adam = optimizers.Adam(lr = 0.0001, beta_1 = 0.9, beta_2 = 0.999, epsilon = None, decay = 0.0, amsgrad = False)
        self.expert_copy.compile(loss = losses.categorical_crossentropy, 
                                 optimizer = adam,
                                 metrics = [metrics.mae, metrics.categorical_accuracy])

    def one_hot(self, data, num_c):
        targets = data.reshape(-1)
        return np.eye(num_c)[targets]

    def run_expert(self, env, render=False):
        # Generates an episode by running the expert policy on the given env.
        return Imitation.generate_episode(self.expert, env, render)

    def run_model(self, env, render=False):
        # Generates an episode by running the cloned policy on the given env.
        return Imitation.generate_episode(self.model, env, render)

    # @staticmethod
    def generate_episode(self, model, env, render=False):
        # Generates an episode by running the given model on the given env.
        # Returns:
        # - a list of states, indexed by time step
        # - a list of actions, indexed by time step
        # - a list of rewards, indexed by time step
        # TODO: Implement this method.
        states = []
        actions = []
        rewards = []

        done = False
        obs = env.reset()

        while (not done):

            acts = model.predict(np.reshape(obs, (1,8)))  # 4 float out for action
            action_chosen = np.argmax(acts)
            oh_vec = self.one_hot(action_chosen, self.num_acts)  # returns a np array in a list
            next_obs, reward, done, _ = env.step(action_chosen)

            states.append(obs)
            actions.append(oh_vec[0].astype(int))
            rewards.append(reward)

            obs = next_obs

        return states, actions, rewards
    
    def train(self, env, name_h5, name_json, num_episodes=100, num_epochs=50, render=False):
        # Trains the model on training data generated by the expert policy.
        # Args:
        # - env: The environment to run the expert policy on. 
        # - num_episodes: # episodes to be generated by the expert.
        # - num_epochs: # epochs to train on the data generated by the expert.
        # - render: Whether to render the environment.
        # Returns the final loss and accuracy.
        # TODO: Implement this method. It may be helpful to call the class
        #       method run_expert() to generate training data.

        # generate training data
        states = []
        actions = []
        rewards = []

        # define model
        model = self.expert_copy

        for i in range (0, num_episodes):
            print ('generating episode: ', i)
            s, a, r = self.generate_episode(self.expert, self.env)
            states += s
            actions += a
            rewards += r

        # train
        for i in range (0, num_epochs):  # number of epochs
            print('epoch: ', i)

            for j in range (0, len(states)): 
                state = states[j]
                action = actions[j]
                reward = rewards[j]

                model.fit(np.reshape(state, (1,8)), np.reshape(action, (1,4)), verbose=0)

        loss = 0
        acc = 0

        print ('saving model and weights...')
        model.save_weights(name_h5, overwrite=True)
        with open(name_json, "w") as outfile:
            json.dump(model.to_json(), outfile)

        return loss, acc


def parse_arguments():
    # Command-line flags are defined here.
    parser = argparse.ArgumentParser()
    parser.add_argument('--model-config-path', dest='model_config_path',
                        type=str, default='LunarLander-v2-config.json',
                        help="Path to the model config file.")
    parser.add_argument('--expert-weights-path', dest='expert_weights_path',
                        type=str, default='LunarLander-v2-weights.h5',
                        help="Path to the expert weights file.")

    # https://stackoverflow.com/questions/15008758/parsing-boolean-values-with-argparse
    parser_group = parser.add_mutually_exclusive_group(required=False)
    parser_group.add_argument('--render', dest='render',
                              action='store_true',
                              help="Whether to render the environment.")
    parser_group.add_argument('--no-render', dest='render',
                              action='store_false',
                              help="Whether to render the environment.")
    parser.set_defaults(render=False)

    return parser.parse_args()


def main(args):
    # Parse command-line arguments.
    args = parse_arguments()
    model_config_path = args.model_config_path
    expert_weights_path = args.expert_weights_path
    render = args.render
    

    
    # TODO: Train cloned models using imitation learning, and record their
    #       performance.

    model_config_path = 'LunarLander-v2-config.json'
    model_weights_path = 'LunarLander-v2-weights.h5'

    # new class
    imitate = Imitation(model_config_path, model_weights_path)
    imitate.train(env=imitate.env, name_h5='1_episode.h5', name_json='1_episode.json', num_episodes=1, num_epochs=50, render=False)
    imitate.train(env=imitate.env, name_h5='10_episode.h5', name_json='10_episode.json', num_episodes=10, num_epochs=50, render=False)
    imitate.train(env=imitate.env, name_h5='50_episode.h5', name_json='50_episode.json', num_episodes=50, num_epochs=50, render=False)
    imitate.train(env=imitate.env, name_h5='100_episode.h5', name_json='100_episode.json', num_episodes=100, num_epochs=50, render=False)




    # 8 observations: x, y, vx, vy, angle, angv, left leg contact, right leg contact
    # 4 actions: none, left engine, main engine, right engine






if __name__ == '__main__':
    main(sys.argv)
