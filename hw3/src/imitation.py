import keras
from keras import optimizers
from keras.layers import Activation
from keras import losses
from keras import metrics

import sys
import argparse
import numpy as np
import random
import gym
import pdb
import json
import matplotlib.pyplot as plt


class Imitation():
    def __init__(self, model_config_path, expert_weights_path):
        # Load the expert model.
        with open(model_config_path, 'r') as f:
            self.expert = keras.models.model_from_json(f.read())
        self.expert.load_weights(expert_weights_path)
        
        # Initialize the cloned model (to be trained).
        with open(model_config_path, 'r') as f:
            self.model = keras.models.model_from_json(f.read())

        # Create the environment.
        self.env = gym.make('LunarLander-v2')
        self.num_obs = self.env.observation_space.shape[0]
        self.num_acts = 4

        # model
        adam = optimizers.Adam(lr = 0.0001, beta_1 = 0.9, beta_2 = 0.999, epsilon = None, decay = 0.0, amsgrad = False)
        self.model.compile(loss = losses.categorical_crossentropy, 
                           optimizer = adam,
                           metrics = [metrics.mae, metrics.categorical_accuracy])

    def one_hot(self, data, num_c):
        targets = data.reshape(-1)
        return np.eye(num_c)[targets]

    def run_expert(self, env, render=False):
        # Generates an episode by running the expert policy on the given env.
        return Imitation.generate_episode(self.expert, env, render)

    def run_model(self, env, render=False):
        # Generates an episode by running the cloned policy on the given env.
        return Imitation.generate_episode(self.model, env, render)

    # @staticmethod
    def generate_episode(self, model, env, render=False):
        # Generates an episode by running the given model on the given env.
        # Returns:
        # - a list of states, indexed by time step
        # - a list of actions, indexed by time step
        # - a list of rewards, indexed by time step
        # TODO: Implement this method.
        states = []
        actions = []
        rewards = []

        done = False
        obs = env.reset()

        while (not done):

            acts = model.predict(np.reshape(obs, (1,8)))  # 4 float out for action
            action_chosen = np.argmax(acts)
            oh_vec = self.one_hot(action_chosen, self.num_acts)  # returns a np array in a list
            next_obs, reward, done, _ = env.step(action_chosen)

            states.append(obs)
            actions.append(oh_vec[0].astype(int))
            rewards.append(reward)

            obs = next_obs

        return states, actions, rewards
    
    def train(self, env, name_h5, name_json, num_episodes=100, num_epochs=50, render=False):
        # Trains the model on training data generated by the expert policy.
        # Args:
        # - env: The environment to run the expert policy on. 
        # - num_episodes: # episodes to be generated by the expert.
        # - num_epochs: # epochs to train on the data generated by the expert.
        # - render: Whether to render the environment.
        # Returns the final loss and accuracy.
        # TODO: Implement this method. It may be helpful to call the class
        #       method run_expert() to generate training data.

        # generate training data
        states = []
        actions = []
        rewards = []

        # define model
        model = self.model

        for i in range (0, num_episodes):
            print ('generating episode: ', i)
            s, a, r = self.generate_episode(self.expert, self.env)
            states += s
            actions += a
            rewards += r

        # train
        for i in range (0, num_epochs):  # number of epochs
            print('epoch: ', i)

            for j in range (0, len(states)): 
                state = states[j]
                action = actions[j]
                reward = rewards[j]

                model.fit(np.reshape(state, (1,8)), np.reshape(action, (1,4)), verbose=1)

        loss = 0
        acc = 0

        print ('saving model and weights...')
        model.save_weights(name_h5, overwrite=True)
        with open(name_json, "w") as outfile:
            json.dump(model.to_json(), outfile)

        return loss, acc

    def test(self, model):
        # Evaluate the performance of your agent over 100 episodes, by calculating cummulative rewards for the 100 episodes.
        # Here you need to interact with the environment, irrespective of whether you are using a memory. 
        # for plotting
        rewards = []

        # load pre-trained model
        print ('loaded network')
        for e in range (0, 50):
            done = False
            obs = self.env.reset()
            print ('testing episode: ', e)

            # reward
            rt = 0
            # self.epsilon = 0.05

            while (done == False):
                # network forward pass

                obs = obs.reshape((1, self.num_obs))
                q_values = model.predict(obs)

                action = np.argmax(q_values)  # greedy works a lot better

                next_obs, reward, done, _ = self.env.step(action)
                # self.env.render()

                obs = next_obs

                rt += reward
            print ('episode: ', e, ' reward is: ', rt)
            rewards.append(rt)

        print (rewards)
        print ('mean: ', sum(rewards)/len(rewards))
        print ('std: ', np.std(np.array(rewards)))

        pdb.set_trace()
        plt.plot(rewards)
        plt.xlabel('# of Episodes')
        plt.ylabel('Reward')
        plt.ylim(-10, 250)
        plt.show()


def parse_arguments():
    # Command-line flags are defined here.
    parser = argparse.ArgumentParser()
    parser.add_argument('--model-config-path', dest='model_config_path',
                        type=str, default='LunarLander-v2-config.json',
                        help="Path to the model config file.")
    parser.add_argument('--expert-weights-path', dest='expert_weights_path',
                        type=str, default='LunarLander-v2-weights.h5',
                        help="Path to the expert weights file.")

    # https://stackoverflow.com/questions/15008758/parsing-boolean-values-with-argparse
    parser_group = parser.add_mutually_exclusive_group(required=False)
    parser_group.add_argument('--render', dest='render',
                              action='store_true',
                              help="Whether to render the environment.")
    parser_group.add_argument('--no-render', dest='render',
                              action='store_false',
                              help="Whether to render the environment.")
    parser.set_defaults(render=False)

    return parser.parse_args()


def main(args):
    # Parse command-line arguments.
    args = parse_arguments()
    model_config_path = args.model_config_path
    expert_weights_path = args.expert_weights_path
    render = args.render
    

    
    # TODO: Train cloned models using imitation learning, and record their
    #       performance.

    model_config_path = 'LunarLander-v2-config.json'
    model_weights_path = 'LunarLander-v2-weights.h5'

    # new class
    imitate = Imitation(model_config_path, model_weights_path)
    imitate.train(env=imitate.env, name_h5='crap.h5', name_json='crap.json', num_episodes=1, num_epochs=50, render=False)
    # imitate.train(env=imitate.env, name_h5='10_episode.h5', name_json='10_episode.json', num_episodes=10, num_epochs=50, render=False)
    # imitate.train(env=imitate.env, name_h5='50_episode.h5', name_json='50_episode.json', num_episodes=50, num_epochs=50, render=False)
    # imitate.train(env=imitate.env, name_h5='100_episode.h5', name_json='100_episode.json', num_episodes=100, num_epochs=50, render=False)

    # with open(model_config_path, 'r') as f:
    #     trained_model = keras.models.model_from_json(f.read())
    # trained_model.load_weights('1_episode.h5')

    # # imitate.test(trained_model)



    # 8 observations: x, y, vx, vy, angle, angv, left leg contact, right leg contact
    # 4 actions: none, left engine, main engine, right engine



if __name__ == '__main__':
    main(sys.argv)
